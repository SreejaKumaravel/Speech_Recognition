{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15ZIqHMyK8yS"
      },
      "outputs": [],
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install -q bitsandbytes datasets accelerate\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft"
      ],
      "metadata": {
        "id": "kxpGL5-BLCNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import nltk\n",
        "import csv\n",
        "#import fastwer\n",
        "from jiwer import cer\n",
        "#import gradio as gr\n",
        "from transformers import (\n",
        "    AutomaticSpeechRecognitionPipeline,\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "\n",
        "peft_model_id = \"yaygomii/FYP_Whisper_PEFT_TAMIL\"\n",
        "language = \"Tamil\"\n",
        "task = \"transcribe\"\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
        "feature_extractor = processor.feature_extractor\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
        "\n",
        "def transcribe(audio):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "1QvAnnKYh39J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PVTiKRdTLLoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transcriptio of the each audio file saved as each Text file in the given save_dir**"
      ],
      "metadata": {
        "id": "KZXF2yaaLdw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to audio directory\n",
        "src = '/content/drive/MyDrive/Project 2/Testing'\n",
        "\n",
        "# Directory to save transcription text files\n",
        "save_dir = '/content/drive/MyDrive/SSNCSE/Whisper_PEFT/Peft/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Set the specific audio number you want to transcribe\n",
        "target_audio_number = \"50\"\n",
        "\n",
        "# Match files that start with \"Audio - 45_\" and end with \".wav\"\n",
        "audio_files = sorted(\n",
        "    [f for f in os.listdir(src) if f.endswith(\".wav\") and f.startswith(f\"Audio - {target_audio_number}_\")],\n",
        "    key=lambda x: int(''.join(filter(str.isdigit, x)))\n",
        ")\n",
        "\n",
        "# Transcription loop for matching files only\n",
        "for file in audio_files:\n",
        "    path = os.path.join(src, file)\n",
        "    print(f\"Transcribing: {path}\")\n",
        "\n",
        "    # Transcribe using your model\n",
        "    transcription = pipe(path, return_timestamps=True)\n",
        "\n",
        "    print(\"Transcription:\", transcription)\n",
        "\n",
        "    # Generate .txt file name from audio file name\n",
        "    filename_wo_ext = os.path.splitext(file)[0]\n",
        "    txt_filename = f\"{filename_wo_ext}.txt\"\n",
        "    txt_path = os.path.join(save_dir, txt_filename)\n",
        "\n",
        "    # Write transcription to the file\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "        txt_file.write(transcription['text'])\n"
      ],
      "metadata": {
        "id": "Vc27ag0NRCgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}